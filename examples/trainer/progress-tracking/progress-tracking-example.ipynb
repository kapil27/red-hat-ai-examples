{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155e8874",
   "metadata": {},
   "source": [
    "# Real-Time Progress Tracking with TransformersTrainer\n",
    "\n",
    "This notebook demonstrates how to monitor distributed training progress in real-time using `TransformersTrainer` from Kubeflow Trainer v2 on Red Hat OpenShift AI.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this example, we fine-tune the **Qwen 2.5 1.5B Instruct** model on the **Stanford Alpaca** instruction-following dataset. The training runs on 2 GPU nodes with automatic progress tracking enabled, allowing you to monitor training metrics in real-time from the OpenShift AI Dashboard.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Automatic Progress Tracking** | TransformersTrainer auto-injects a `KubeflowProgressCallback` that exposes training metrics via HTTP |\n",
    "| **Real-Time Metrics** | View current step, epoch, loss, and estimated time remaining in the OpenShift AI Dashboard |\n",
    "| **PVC-Based Checkpointing** | Save model checkpoints to a shared PersistentVolumeClaim for durability and resume capability |\n",
    "| **Distributed Training** | Run training across multiple GPU nodes using PyTorch's DistributedDataParallel (DDP) |\n",
    "\n",
    "### Model Details\n",
    "\n",
    "**Qwen 2.5 1.5B Instruct** is a compact instruction-tuned language model from the Qwen family:\n",
    "- **Parameters:** 1.5 billion\n",
    "- **Context Length:** 32K tokens\n",
    "- **Languages:** Multilingual with strong English and Chinese support\n",
    "- **Use Case:** Ideal for instruction-following, chat, and text generation tasks\n",
    "- **Why this model?** Small enough to train quickly for demonstration, yet powerful enough for real-world tasks\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. **OpenShift AI Cluster** with Kubeflow Trainer v2 enabled\n",
    "2. **Workbench** running Python 3.12+ with GPU access\n",
    "3. **Environment Variables:**\n",
    "   - `OPENSHIFT_API_URL` - Your OpenShift API server URL\n",
    "   - `NOTEBOOK_USER_TOKEN` - Authentication token for API access\n",
    "4. **Shared PVC** named `shared` mounted at `/opt/app-root/src/shared` in the workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306a076",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Install the Kubeflow SDK and required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c80093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/knema/Library/Python/3.9/lib/python/site-packages (4.4.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface_hub in /Users/knema/Library/Python/3.9/lib/python/site-packages (1.2.1)\n",
      "Requirement already satisfied: filelock in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: packaging in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /Users/knema/Library/Python/3.9/lib/python/site-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/knema/Library/Python/3.9/lib/python/site-packages (from huggingface_hub) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/knema/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/knema/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/knema/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/knema/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2026.1.15-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: psutil in /Users/knema/Library/Python/3.9/lib/python/site-packages (from accelerate) (7.0.0)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.0.0->accelerate)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=2.0.0->accelerate)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->accelerate)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.0.0->accelerate)\n",
      "  Downloading markupsafe-3.0.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/knema/Library/Python/3.9/lib/python/site-packages (from typer-slim->huggingface_hub) (8.1.8)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading regex-2026.1.15-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, safetensors, regex, networkx, MarkupSafe, jinja2, huggingface_hub, torch, tokenizers, transformers, accelerate\n",
      "\u001b[2K  Attempting uninstall: huggingface_hubm━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.2.1━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.2.1:━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.2.1━━━━━━━━━━\u001b[0m \u001b[32m 4/12\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [accelerate]2\u001b[0m [accelerate]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 accelerate-1.10.1 huggingface_hub-0.36.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 regex-2026.1.15 safetensors-0.7.0 sympy-1.14.0 tokenizers-0.22.2 torch-2.8.0 transformers-4.57.6\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0\n",
      "  Cloning https://github.com/opendatahub-io/kubeflow-sdk.git (to revision v0.2.1+rhai0) to /private/var/folders/g7/rfmmh71x7fzbvrv5lwk93p140000gn/T/pip-install-ify2n40a/kubeflow_f4a9e2b3c3954d1c911a0eceba191346\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/opendatahub-io/kubeflow-sdk.git /private/var/folders/g7/rfmmh71x7fzbvrv5lwk93p140000gn/T/pip-install-ify2n40a/kubeflow_f4a9e2b3c3954d1c911a0eceba191346\n",
      "  Running command git checkout -q 7b3d014a8434af6f55897a274a6aa68af3fdf87f\n",
      "  Resolved https://github.com/opendatahub-io/kubeflow-sdk.git to commit 7b3d014a8434af6f55897a274a6aa68af3fdf87f\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting kubeflow-katib-api>=0.19.0 (from kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading kubeflow_katib_api-0.19.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting kubeflow-trainer-api>=2.0.0 (from kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading kubeflow_trainer_api-2.1.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting kubernetes>=27.2.0 (from kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading kubernetes-35.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pydantic>=2.10.0 (from kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting certifi>=14.05.14 (from kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting six>=1.9.0 (from kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting python-dateutil>=2.5.3 (from kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyyaml>=5.4.1 (from kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading pyyaml-6.0.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting requests (from kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting urllib3!=2.6.0,>=1.24.2 (from kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.10.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.10.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading pydantic_core-2.41.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-extensions>=4.14.1 (from pydantic>=2.10.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.10.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading charset_normalizer-3.4.4-cp39-cp39-macosx_10_9_universal2.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib->kubernetes>=27.2.0->kubeflow@ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Downloading kubeflow_katib_api-0.19.0-py3-none-any.whl (642 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m643.0/643.0 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubeflow_trainer_api-2.1.0-py3-none-any.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.4/739.4 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-35.0.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp39-cp39-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading pyyaml-6.0.3-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp39-cp39-macosx_10_9_universal2.whl (209 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Building wheels for collected packages: kubeflow\n",
      "  Building wheel for kubeflow (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kubeflow: filename=kubeflow-0.2.1+rhai0-py3-none-any.whl size=164054 sha256=c9cfa658ac27fe35cc7d5c19ff31a5923b072e5d2611027a5d00db9c7cae9a76\n",
      "  Stored in directory: /private/var/folders/g7/rfmmh71x7fzbvrv5lwk93p140000gn/T/pip-ephem-wheel-cache-dxf8fkoz/wheels/61/12/93/10419fccf4c8819b6b40889e9649e75a171388a2e2526a9b7e\n",
      "Successfully built kubeflow\n",
      "Installing collected packages: durationpy, websocket-client, urllib3, typing-extensions, six, pyyaml, oauthlib, idna, charset_normalizer, certifi, annotated-types, typing-inspection, requests, python-dateutil, pydantic-core, requests-oauthlib, pydantic, kubernetes, kubeflow-trainer-api, kubeflow-katib-api, kubeflow\n",
      "\u001b[2K  Attempting uninstall: durationpy\n",
      "\u001b[2K    Found existing installation: durationpy 0.10\n",
      "\u001b[2K    Uninstalling durationpy-0.10:\n",
      "\u001b[2K      Successfully uninstalled durationpy-0.10\n",
      "\u001b[2K  Attempting uninstall: websocket-client\n",
      "\u001b[2K    Found existing installation: websocket-client 1.9.0\n",
      "\u001b[2K    Uninstalling websocket-client-1.9.0:\n",
      "\u001b[2K      Successfully uninstalled websocket-client-1.9.0\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 2.3.0\n",
      "\u001b[2K    Uninstalling urllib3-2.3.0:\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.3.0\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.0\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0━━━━━━━━━━━\u001b[0m \u001b[32m 3/21\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: sixm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/21\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: six 1.17.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/21\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling six-1.17.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/21\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/21\u001b[0m [six]xtensions]\n",
      "\u001b[2K  Attempting uninstall: pyyaml━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/21\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/21\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/21\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.3━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/21\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: oauthlib[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K    Found existing installation: oauthlib 3.3.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K    Uninstalling oauthlib-3.3.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K      Successfully uninstalled oauthlib-3.3.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K  Attempting uninstall: idna[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K    Found existing installation: idna 3.11━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K    Uninstalling idna-3.11:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K      Successfully uninstalled idna-3.11━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.4━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.4:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.4━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [pyyaml]\n",
      "\u001b[2K  Attempting uninstall: certifim╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: certifi 2025.11.12━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling certifi-2025.11.12:0m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.11.12━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: annotated-types━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: annotated-types 0.7.0━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling annotated-types-0.7.0:━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled annotated-types-0.7.0━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: typing-inspection━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: typing-inspection 0.4.2━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling typing-inspection-0.4.2:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled typing-inspection-0.4.2━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: requestsm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: requests 2.32.5━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling requests-2.32.5:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.5━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: pydantic-corem━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.41.5━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling pydantic_core-2.41.5:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.41.5━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: requests-oauthlib━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: requests-oauthlib 2.0.0━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling requests-oauthlib-2.0.0:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled requests-oauthlib-2.0.0━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: pydanticm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: pydantic 2.12.5━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling pydantic-2.12.5:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.12.5━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: kubernetes━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m16/21\u001b[0m [pydantic]lizer]\n",
      "\u001b[2K    Found existing installation: kubernetes 34.1.090m━━━━━━━━━\u001b[0m \u001b[32m16/21\u001b[0m [pydantic]\n",
      "\u001b[2K    Uninstalling kubernetes-34.1.0:━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [kubernetes]\n",
      "\u001b[2K      Successfully uninstalled kubernetes-34.1.00m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [kubernetes]\n",
      "\u001b[2K  Attempting uninstall: kubeflow-trainer-api\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [kubernetes]\n",
      "\u001b[2K    Found existing installation: kubeflow_trainer_api 2.1.0━━━\u001b[0m \u001b[32m17/21\u001b[0m [kubernetes]\n",
      "\u001b[2K    Uninstalling kubeflow_trainer_api-2.1.0:m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [kubernetes]\n",
      "\u001b[2K      Successfully uninstalled kubeflow_trainer_api-2.1.0━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [kubernetes]\n",
      "\u001b[2K  Rolling back uninstall of kubeflow_trainer_api90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m18/21\u001b[0m [kubeflow-trainer-api]\n",
      "\u001b[2K  Moving to /Users/knema/Library/Caches/com.apple.python/Users/knema/Library/Python/3.9/lib/python/site-packages/kubeflow_trainer_api/\n",
      "   from /Users/knema/Library/Caches/com.apple.python/Users/knema/Library/Python/3.9/lib/python/site-packages/~ubeflow_trainer_api\n",
      "\u001b[2K  Moving to /Users/knema/Library/Python/3.9/lib/python/site-packages/kubeflow_trainer_api-2.1.0.dist-info/\n",
      "   from /Users/knema/Library/Python/3.9/lib/python/site-packages/~ubeflow_trainer_api-2.1.0.dist-info\n",
      "\u001b[2K  Moving to /Users/knema/Library/Python/3.9/lib/python/site-packages/kubeflow_trainer_api/w-trainer-api]\n",
      "   from /Users/knema/Library/Python/3.9/lib/python/site-packages/~ubeflow_trainer_api\n",
      "   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m18/21\u001b[0m [kubeflow-trainer-api]\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/py_compile.py\", line 144, in compile\n",
      "    code = loader.source_to_code(source_bytes, dfile or file,\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 913, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/kubeflow_trainer_api/models/trainer_v1alpha1_pod_spec_override.py\", line 36\n",
      "    <<<<<<< HEAD\n",
      "    ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/compileall.py\", line 238, in compile_file\n",
      "    ok = py_compile.compile(fullname, cfile, dfile, True,\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/py_compile.py\", line 150, in compile\n",
      "    raise py_exc\n",
      "py_compile.PyCompileError:   File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/kubeflow_trainer_api/models/trainer_v1alpha1_pod_spec_override.py\", line 36\n",
      "    <<<<<<< HEAD\n",
      "    ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/req_command.py\", line 85, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/pip/_internal/commands/install.py\", line 458, in run\n",
      "    installed = install_given_reqs(\n",
      "  File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/pip/_internal/req/__init__.py\", line 84, in install_given_reqs\n",
      "    requirement.install(\n",
      "  File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/pip/_internal/req/req_install.py\", line 781, in install\n",
      "    install_wheel(\n",
      "  File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/pip/_internal/operations/install/wheel.py\", line 737, in install_wheel\n",
      "    _install_wheel(\n",
      "  File \"/Users/knema/Library/Python/3.9/lib/python/site-packages/pip/_internal/operations/install/wheel.py\", line 623, in _install_wheel\n",
      "    success = compileall.compile_file(path, force=True, quiet=True)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/compileall.py\", line 255, in compile_file\n",
      "    msg = err.msg.encode(sys.stdout.encoding,\n",
      "TypeError: encode() argument 'encoding' must be str, not None\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m18/21\u001b[0m [kubeflow-trainer-api]\n",
      "\u001b[1A\u001b[2KDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipykernel\n",
      "  Downloading ipykernel-6.31.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting appnope>=0.1.2 (from ipykernel)\n",
      "  Downloading appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\n",
      "Collecting comm>=0.1.1 (from ipykernel)\n",
      "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting debugpy>=1.6.5 (from ipykernel)\n",
      "  Downloading debugpy-1.8.19-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting ipython>=7.23.1 (from ipykernel)\n",
      "  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting jupyter-client>=8.0.0 (from ipykernel)\n",
      "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
      "  Downloading jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
      "  Downloading matplotlib_inline-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting nest-asyncio>=1.4 (from ipykernel)\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting packaging>=22 (from ipykernel)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting psutil>=5.7 (from ipykernel)\n",
      "  Downloading psutil-7.2.1-cp36-abi3-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Collecting pyzmq>=25 (from ipykernel)\n",
      "  Downloading pyzmq-27.1.0-cp39-cp39-macosx_10_15_universal2.whl.metadata (6.0 kB)\n",
      "Collecting tornado>=6.2 (from ipykernel)\n",
      "  Downloading tornado-6.5.4-cp39-abi3-macosx_10_9_universal2.whl.metadata (2.8 kB)\n",
      "Collecting traitlets>=5.4.0 (from ipykernel)\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting decorator (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading prompt_toolkit-3.0.52-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting stack-data (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting typing-extensions (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting exceptiongroup (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading exceptiongroup-1.3.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
      "  Downloading wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
      "  Downloading parso-0.8.5-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting importlib-metadata>=4.8.3 (from jupyter-client>=8.0.0->ipykernel)\n",
      "  Downloading importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from jupyter-client>=8.0.0->ipykernel)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=4.8.3->jupyter-client>=8.0.0->ipykernel)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel)\n",
      "  Downloading platformdirs-4.4.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
      "  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
      "  Downloading asttokens-3.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel)\n",
      "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading ipykernel-6.31.0-py3-none-any.whl (117 kB)\n",
      "Downloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\n",
      "Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
      "Downloading debugpy-1.8.19-py2.py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.2/808.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n",
      "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading parso-0.8.5-py2.py3-none-any.whl (106 kB)\n",
      "Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
      "Downloading importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Downloading jupyter_core-5.8.1-py3-none-any.whl (28 kB)\n",
      "Downloading matplotlib_inline-0.2.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Downloading platformdirs-4.4.0-py3-none-any.whl (18 kB)\n",
      "Downloading psutil-7.2.1-cp36-abi3-macosx_11_0_arm64.whl (128 kB)\n",
      "Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading pyzmq-27.1.0-cp39-cp39-macosx_10_15_universal2.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading tornado-6.5.4-cp39-abi3-macosx_10_9_universal2.whl (443 kB)\n",
      "Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Downloading exceptiongroup-1.3.1-py3-none-any.whl (16 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading asttokens-3.0.1-py3-none-any.whl (27 kB)\n",
      "Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
      "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pure-eval, ptyprocess, zipp, wcwidth, typing-extensions, traitlets, tornado, six, pyzmq, pygments, psutil, platformdirs, pexpect, parso, packaging, nest-asyncio, executing, decorator, debugpy, comm, asttokens, appnope, stack-data, python-dateutil, prompt-toolkit, matplotlib-inline, jupyter-core, jedi, importlib-metadata, exceptiongroup, jupyter-client, ipython, ipykernel\n",
      "\u001b[2K  Attempting uninstall: pure-eval\n",
      "\u001b[2K    Found existing installation: pure_eval 0.2.3\n",
      "\u001b[2K    Uninstalling pure_eval-0.2.3:\n",
      "\u001b[2K      Successfully uninstalled pure_eval-0.2.3\n",
      "\u001b[2K  Attempting uninstall: ptyprocess\n",
      "\u001b[2K    Found existing installation: ptyprocess 0.7.0\n",
      "\u001b[2K    Uninstalling ptyprocess-0.7.0:\n",
      "\u001b[2K      Successfully uninstalled ptyprocess-0.7.0\n",
      "\u001b[2K  Attempting uninstall: zipp\n",
      "\u001b[2K    Found existing installation: zipp 3.23.0\n",
      "\u001b[2K    Uninstalling zipp-3.23.0:\n",
      "\u001b[2K      Successfully uninstalled zipp-3.23.0\n",
      "\u001b[2K  Attempting uninstall: wcwidth\n",
      "\u001b[2K    Found existing installation: wcwidth 0.2.13\n",
      "\u001b[2K    Uninstalling wcwidth-0.2.13:\n",
      "\u001b[2K      Successfully uninstalled wcwidth-0.2.13\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.0\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0━━━━━━━━━━━\u001b[0m \u001b[32m 4/33\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: traitlets━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/33\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: traitlets 5.14.3━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/33\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling traitlets-5.14.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/33\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled traitlets-5.14.3━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/33\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tornado0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/33\u001b[0m [traitlets]ons]\n",
      "\u001b[2K    Found existing installation: tornado 6.5.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/33\u001b[0m [traitlets]\n",
      "\u001b[2K    Uninstalling tornado-6.5.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/33\u001b[0m [traitlets]\n",
      "\u001b[2K      Successfully uninstalled tornado-6.5.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/33\u001b[0m [traitlets]\n",
      "\u001b[2K  Attempting uninstall: six0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/33\u001b[0m [tornado]\n",
      "\u001b[2K    Found existing installation: six 1.17.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/33\u001b[0m [tornado]\n",
      "\u001b[2K    Uninstalling six-1.17.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/33\u001b[0m [tornado]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/33\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: pyzmq0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/33\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: pyzmq 26.4.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/33\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling pyzmq-26.4.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/33\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled pyzmq-26.4.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/33\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: pygments━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/33\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: Pygments 2.19.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/33\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling Pygments-2.19.1:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K      Successfully uninstalled Pygments-2.19.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K  Attempting uninstall: psutil0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K    Found existing installation: psutil 7.0.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K    Uninstalling psutil-7.0.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K      Successfully uninstalled psutil-7.0.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K  Attempting uninstall: platformdirs━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K    Found existing installation: platformdirs 4.3.8━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K    Uninstalling platformdirs-4.3.8:━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K      Successfully uninstalled platformdirs-4.3.8━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K  Attempting uninstall: pexpect0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K    Found existing installation: pexpect 4.9.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [pygments]\n",
      "\u001b[2K    Uninstalling pexpect-4.9.0:╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K      Successfully uninstalled pexpect-4.9.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K  Attempting uninstall: parso[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K    Found existing installation: parso 0.8.4━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K    Uninstalling parso-0.8.4:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K      Successfully uninstalled parso-0.8.4━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K  Attempting uninstall: packaging[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K    Uninstalling packaging-25.0:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K  Attempting uninstall: nest-asynciom━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K    Found existing installation: nest-asyncio 1.6.0━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K    Uninstalling nest-asyncio-1.6.0:m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/33\u001b[0m [pexpect]\n",
      "\u001b[2K      Successfully uninstalled nest-asyncio-1.6.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/33\u001b[0m [nest-asyncio]\n",
      "\u001b[2K  Attempting uninstall: executing[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/33\u001b[0m [nest-asyncio]\n",
      "\u001b[2K    Found existing installation: executing 2.2.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/33\u001b[0m [nest-asyncio]\n",
      "\u001b[2K    Uninstalling executing-2.2.0:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/33\u001b[0m [nest-asyncio]\n",
      "\u001b[2K      Successfully uninstalled executing-2.2.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/33\u001b[0m [nest-asyncio]\n",
      "\u001b[2K  Attempting uninstall: decorator[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/33\u001b[0m [nest-asyncio]\n",
      "\u001b[2K    Found existing installation: decorator 5.2.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/33\u001b[0m [nest-asyncio]\n",
      "\u001b[2K    Uninstalling decorator-5.2.1:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/33\u001b[0m [nest-asyncio]\n",
      "\u001b[2K      Successfully uninstalled decorator-5.2.1m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/33\u001b[0m [decorator]\n",
      "\u001b[2K  Attempting uninstall: debugpy1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/33\u001b[0m [decorator]\n",
      "\u001b[2K    Found existing installation: debugpy 1.8.14━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/33\u001b[0m [decorator]\n",
      "\u001b[2K    Uninstalling debugpy-1.8.14:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/33\u001b[0m [decorator]\n",
      "\u001b[2K      Successfully uninstalled debugpy-1.8.14━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/33\u001b[0m [decorator]\n",
      "\u001b[2K  Attempting uninstall: comm━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K    Found existing installation: comm 0.2.2m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K    Uninstalling comm-0.2.2:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K      Successfully uninstalled comm-0.2.290m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K  Attempting uninstall: asttokensm╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K    Found existing installation: asttokens 3.0.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K    Uninstalling asttokens-3.0.0:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K      Successfully uninstalled asttokens-3.0.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K  Attempting uninstall: appnope91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K    Found existing installation: appnope 0.1.4━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K    Uninstalling appnope-0.1.4:91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K      Successfully uninstalled appnope-0.1.4━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K  Attempting uninstall: stack-data╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K    Found existing installation: stack-data 0.6.3━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K    Uninstalling stack-data-0.6.3:╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/33\u001b[0m [debugpy]\n",
      "\u001b[2K      Successfully uninstalled stack-data-0.6.3m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m22/33\u001b[0m [stack-data]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m22/33\u001b[0m [stack-data]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0━━\u001b[0m \u001b[32m22/33\u001b[0m [stack-data]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m22/33\u001b[0m [stack-data]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0━━━━\u001b[0m \u001b[32m22/33\u001b[0m [stack-data]\n",
      "\u001b[2K  Attempting uninstall: prompt-toolkitm╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m22/33\u001b[0m [stack-data]\n",
      "\u001b[2K    Found existing installation: prompt_toolkit 3.0.51━━━━━━━━\u001b[0m \u001b[32m22/33\u001b[0m [stack-data]\n",
      "\u001b[2K    Uninstalling prompt_toolkit-3.0.51:╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m22/33\u001b[0m [stack-data]\n",
      "\u001b[2K      Successfully uninstalled prompt_toolkit-3.0.51━━━━━━━━━━\u001b[0m \u001b[32m22/33\u001b[0m [stack-data]\n",
      "\u001b[2K  Attempting uninstall: matplotlib-inline\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m24/33\u001b[0m [prompt-toolkit]\n",
      "\u001b[2K    Found existing installation: matplotlib-inline 0.1.7━━━━━━\u001b[0m \u001b[32m24/33\u001b[0m [prompt-toolkit]\n",
      "\u001b[2K    Uninstalling matplotlib-inline-0.1.7:m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m24/33\u001b[0m [prompt-toolkit]\n",
      "\u001b[2K      Successfully uninstalled matplotlib-inline-0.1.7━━━━━━━━\u001b[0m \u001b[32m24/33\u001b[0m [prompt-toolkit]\n",
      "\u001b[2K  Attempting uninstall: jupyter-corem\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m24/33\u001b[0m [prompt-toolkit]\n",
      "\u001b[2K    Found existing installation: jupyter_core 5.8.1m━━━━━━━━━━\u001b[0m \u001b[32m24/33\u001b[0m [prompt-toolkit]\n",
      "\u001b[2K    Uninstalling jupyter_core-5.8.1:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m24/33\u001b[0m [prompt-toolkit]\n",
      "\u001b[2K      Successfully uninstalled jupyter_core-5.8.1\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m26/33\u001b[0m [jupyter-core]\n",
      "\u001b[2K  Attempting uninstall: jedi━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m26/33\u001b[0m [jupyter-core]\n",
      "\u001b[2K    Found existing installation: jedi 0.19.2╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m26/33\u001b[0m [jupyter-core]\n",
      "\u001b[2K    Uninstalling jedi-0.19.2:━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]ore]\n",
      "\u001b[2K      Successfully uninstalled jedi-0.19.291m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K  Attempting uninstall: importlib-metadata0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K    Found existing installation: importlib_metadata 8.7.0━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K    Uninstalling importlib_metadata-8.7.0:91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K      Successfully uninstalled importlib_metadata-8.7.0━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K  Attempting uninstall: exceptiongroup0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K    Found existing installation: exceptiongroup 1.3.00m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K    Uninstalling exceptiongroup-1.3.0:0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K      Successfully uninstalled exceptiongroup-1.3.0[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K  Attempting uninstall: jupyter-client0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K    Found existing installation: jupyter_client 8.6.30m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K    Uninstalling jupyter_client-8.6.3:0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K      Successfully uninstalled jupyter_client-8.6.3[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K  Attempting uninstall: ipython━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K    Found existing installation: ipython 8.18.1[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K    Uninstalling ipython-8.18.1:━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K      Successfully uninstalled ipython-8.18.1╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m27/33\u001b[0m [jedi]\n",
      "\u001b[2K  Attempting uninstall: ipykernel━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [ipython]\n",
      "\u001b[2K    Found existing installation: ipykernel 6.29.5m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [ipython]\n",
      "\u001b[2K    Uninstalling ipykernel-6.29.5:━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [ipython]\n",
      "\u001b[2K      Successfully uninstalled ipykernel-6.29.5[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m32/33\u001b[0m [ipykernel]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/33\u001b[0m [ipykernel]33\u001b[0m [ipykernel]\n",
      "\u001b[1A\u001b[2KSuccessfully installed appnope-0.1.4 asttokens-3.0.1 comm-0.2.3 debugpy-1.8.19 decorator-5.2.1 exceptiongroup-1.3.1 executing-2.2.1 importlib-metadata-8.7.1 ipykernel-6.31.0 ipython-8.18.1 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.8.1 matplotlib-inline-0.2.1 nest-asyncio-1.6.0 packaging-25.0 parso-0.8.5 pexpect-4.9.0 platformdirs-4.4.0 prompt-toolkit-3.0.52 psutil-7.2.1 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.19.2 python-dateutil-2.9.0.post0 pyzmq-27.1.0 six-1.17.0 stack-data-0.6.3 tornado-6.5.4 traitlets-5.14.3 typing-extensions-4.15.0 wcwidth-0.2.14 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install datasets transformers accelerate huggingface_hub\n",
    "!python3 -m pip install --force-reinstall --no-cache-dir -U \"kubeflow @ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0\"\n",
    "!python3 -m pip install --force-reinstall --no-cache-dir -U ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b68cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import kubeflow\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from kubeflow.common.types import KubernetesBackendConfig\n",
    "from kubeflow.trainer import TrainerClient\n",
    "from kubeflow.trainer.rhai import TransformersTrainer\n",
    "from kubernetes import client as k8s\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Kubeflow SDK version: {kubeflow.__version__}\")\n",
    "print(\"✅ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e13d5f",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Configure authentication and paths. TransformersTrainer has **progress tracking enabled by default**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication configuration\n",
    "api_server = os.getenv(\"OPENSHIFT_API_URL\")\n",
    "token = os.getenv(\"NOTEBOOK_USER_TOKEN\")\n",
    "\n",
    "if not api_server or not token:\n",
    "    raise RuntimeError(\n",
    "        \"OPENSHIFT_API_URL and NOTEBOOK_USER_TOKEN environment variables are required\"\n",
    "    )\n",
    "\n",
    "# Configure Kubernetes client\n",
    "configuration = k8s.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.verify_ssl = False  # Set to True if using trusted certificates\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# PVC Configuration\n",
    "PVC_NAME = \"shared\"\n",
    "# In the notebook, the shared PVC is mounted at /opt/app-root/src/shared\n",
    "# In training pods, we mount it at /opt/app-root/src\n",
    "NOTEBOOK_SHARED_PATH = \"/opt/app-root/src/shared\"  # Where notebook sees the shared PVC\n",
    "TRAINING_POD_PATH = \"/opt/app-root/src\"  # Where training pods will mount it\n",
    "\n",
    "# Model Configuration - use notebook path for downloading, training path for train_func\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# Paths for notebook (downloading)\n",
    "MODEL_PATH = f\"{NOTEBOOK_SHARED_PATH}/models/qwen2.5-1.5b-instruct\"\n",
    "DATA_PATH = f\"{NOTEBOOK_SHARED_PATH}/data/alpaca_processed\"\n",
    "CHECKPOINTS_PATH = f\"{NOTEBOOK_SHARED_PATH}/checkpoints/progress-tracking\"\n",
    "\n",
    "# Paths for training pods (used in train_func and PodTemplateOverrides)\n",
    "TRAINING_MODEL_PATH = f\"{TRAINING_POD_PATH}/models/qwen2.5-1.5b-instruct\"\n",
    "TRAINING_DATA_PATH = f\"{TRAINING_POD_PATH}/data/alpaca_processed\"\n",
    "TRAINING_CHECKPOINTS_PATH = f\"{TRAINING_POD_PATH}/checkpoints/progress-tracking\"\n",
    "\n",
    "print(f\"API Server: {api_server}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Notebook Model Path: {MODEL_PATH}\")\n",
    "print(f\"Training Pod Model Path: {TRAINING_MODEL_PATH}\")\n",
    "print(f\"Data Path: {DATA_PATH}\")\n",
    "print(f\"Checkpoints Path: {CHECKPOINTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71d2ba",
   "metadata": {},
   "source": [
    "## Download Model and Dataset to Shared PVC\n",
    "\n",
    "Before submitting the training job, we pre-download the model and dataset to the shared PVC. This ensures:\n",
    "- **Offline Training:** Training pods don't need internet access during training\n",
    "- **Faster Startup:** No download delays when training pods start\n",
    "- **Consistency:** All nodes use the same model weights and data\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "We use the **Stanford Alpaca** dataset (`tatsu-lab/alpaca`), a widely-used instruction-following dataset:\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Source** | Stanford University |\n",
    "| **Size** | 52,000 instruction-response pairs |\n",
    "| **Format** | Instruction, optional input, and response |\n",
    "| **Use Case** | Instruction-tuning language models |\n",
    "\n",
    "Each sample follows this structure:\n",
    "```\n",
    "### Instruction:\n",
    "Give three tips for staying healthy.\n",
    "\n",
    "### Response:\n",
    "1. Eat a balanced diet...\n",
    "2. Exercise regularly...\n",
    "3. Get enough sleep...\n",
    "```\n",
    "\n",
    "For this demo, we use a **500-sample subset** to enable quick training (~1 minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model to PVC\n",
    "if os.path.exists(MODEL_PATH) and os.listdir(MODEL_PATH):\n",
    "    print(f\"✅ Model already exists at {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"🔄 Downloading model {MODEL_NAME} to {MODEL_PATH}...\")\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "    # Use fast tokenizer for compatibility\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.save_pretrained(MODEL_PATH, safe_serialization=True)\n",
    "    print(f\"✅ Model saved to {MODEL_PATH}\")\n",
    "    print(f\"📁 Files: {os.listdir(MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "if os.path.exists(DATA_PATH) and os.listdir(DATA_PATH):\n",
    "    print(f\"✅ Dataset already exists at {DATA_PATH}\")\n",
    "else:\n",
    "    print(\"🔄 Downloading and processing Alpaca dataset...\")\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "    # Load subset of Alpaca dataset\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:500]\")\n",
    "\n",
    "    # Load tokenizer for preprocessing\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def format_instruction(example):\n",
    "        if example.get(\"input\"):\n",
    "            text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "        else:\n",
    "            text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
    "        return {\"text\": text}\n",
    "\n",
    "    dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    "    )\n",
    "    tokenized_dataset.save_to_disk(DATA_PATH)\n",
    "    print(f\"✅ Dataset saved to {DATA_PATH}\")\n",
    "\n",
    "print(\"\\n✅ Model and dataset ready on PVC!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db38f5",
   "metadata": {},
   "source": [
    "## Define the Training Function\n",
    "\n",
    "The training function runs inside each training pod as a distributed PyTorch process. TransformersTrainer serializes this function and executes it via `torchrun` on each node.\n",
    "\n",
    "### How Progress Tracking Works\n",
    "\n",
    "When you use `TransformersTrainer` with `enable_progression_tracking=True` (the default):\n",
    "\n",
    "1. **Automatic Instrumentation:** TransformersTrainer injects a `KubeflowProgressCallback` into your HuggingFace `Trainer`\n",
    "2. **HTTP Metrics Server:** A lightweight HTTP server starts on port 28080, exposing metrics as JSON\n",
    "3. **Dashboard Integration:** OpenShift AI Dashboard polls these metrics and displays real-time progress\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `num_train_epochs` | 1 | Complete one pass through the dataset |\n",
    "| `per_device_train_batch_size` | 2 | Samples per GPU per step |\n",
    "| `gradient_accumulation_steps` | 4 | Effective batch size = 2 × 4 × 2 nodes = 16 |\n",
    "| `learning_rate` | 2e-5 | Standard fine-tuning rate |\n",
    "| `save_steps` | 20 | Checkpoint every 20 steps |\n",
    "| `bf16` | True | Use bfloat16 mixed precision |\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Supported Trainers:** Use `transformers.Trainer` or `trl.SFTTrainer` - both are auto-instrumented\n",
    "- **No Manual Setup:** Progress tracking callback is injected automatically\n",
    "- **Local Files Only:** Model and data are loaded from the mounted PVC (no network access needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0feb354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func():\n",
    "    \"\"\"SFT training function using HuggingFace Trainer.\n",
    "\n",
    "    TransformersTrainer automatically:\n",
    "    - Injects KubeflowProgressCallback for real-time metrics\n",
    "    - Applies checkpoint configuration from periodic_checkpoint_config\n",
    "    - Enables auto-resume from latest checkpoint\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # IMPORTANT: Set offline mode BEFORE importing transformers/huggingface_hub\n",
    "    # This prevents the newer huggingface_hub from validating local paths as repo IDs\n",
    "    os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "    os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "    import torch\n",
    "    from datasets import load_from_disk\n",
    "    from transformers import (\n",
    "        AutoConfig,\n",
    "        AutoModelForCausalLM,\n",
    "        DataCollatorForLanguageModeling,\n",
    "        PreTrainedTokenizerFast,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "\n",
    "    rank = int(os.environ.get(\"RANK\", 0))\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "\n",
    "    # Paths on the shared PVC (mounted via pvc:// URI)\n",
    "    model_path = \"/opt/app-root/src/models/qwen2.5-1.5b-instruct\"\n",
    "    data_path = \"/opt/app-root/src/data/alpaca_processed\"\n",
    "    # output_dir is set by TransformersTrainer from the output_dir parameter\n",
    "    output_dir = \"/opt/app-root/src/checkpoints/progress-tracking\"\n",
    "\n",
    "    print(f\"🚀 Starting training on rank {rank}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        print(f\"🔧 GPU: {torch.cuda.get_device_name(local_rank)}\")\n",
    "\n",
    "    # Load tokenizer directly from tokenizer.json file\n",
    "    # This bypasses AutoTokenizer's hub validation that fails with local paths\n",
    "    print(f\"📥 Loading tokenizer from: {model_path}\")\n",
    "    tokenizer_file = os.path.join(model_path, \"tokenizer.json\")\n",
    "    tokenizer_config_file = os.path.join(model_path, \"tokenizer_config.json\")\n",
    "    \n",
    "    # Load tokenizer config to get special tokens\n",
    "    import json\n",
    "    with open(tokenizer_config_file, \"r\") as f:\n",
    "        tokenizer_config = json.load(f)\n",
    "    \n",
    "    tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=tokenizer_file,\n",
    "        eos_token=tokenizer_config.get(\"eos_token\", \"<|endoftext|>\"),\n",
    "        pad_token=tokenizer_config.get(\"pad_token\"),\n",
    "        bos_token=tokenizer_config.get(\"bos_token\"),\n",
    "        unk_token=tokenizer_config.get(\"unk_token\"),\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model config first, then model - avoids hub validation issues\n",
    "    print(f\"📥 Loading model from: {model_path}\")\n",
    "    config_file = os.path.join(model_path, \"config.json\")\n",
    "    with open(config_file, \"r\") as f:\n",
    "        model_config_dict = json.load(f)\n",
    "    \n",
    "    # Extract model_type and pass remaining config\n",
    "    model_type = model_config_dict.pop(\"model_type\")\n",
    "    config = AutoConfig.for_model(model_type, **model_config_dict)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": local_rank},\n",
    "        local_files_only=True,\n",
    "    )\n",
    "\n",
    "    # Load dataset\n",
    "    print(f\"📥 Loading dataset from: {data_path}\")\n",
    "    tokenized_dataset = load_from_disk(data_path)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Training arguments - TransformersTrainer will override save_* settings\n",
    "    # from periodic_checkpoint_config if provided\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=5,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=20,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "\n",
    "    # Trainer - TransformersTrainer automatically injects KubeflowProgressCallback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train - auto-resumes from latest checkpoint if available\n",
    "    trainer.train()\n",
    "\n",
    "    # Save final model (only on rank 0)\n",
    "    if rank == 0:\n",
    "        final_path = f\"{output_dir}/final\"\n",
    "        trainer.save_model(final_path)\n",
    "        tokenizer.save_pretrained(final_path)\n",
    "        print(f\"✅ Final model saved to {final_path}\")\n",
    "\n",
    "    print(f\"✅ Training complete on rank {rank}\")\n",
    "\n",
    "\n",
    "print(\"✅ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ef44d",
   "metadata": {},
   "source": [
    "## Create the Trainer Client\n",
    "\n",
    "Initialize the TrainerClient with authentication configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client with authentication\n",
    "api_client = k8s.ApiClient(configuration)\n",
    "\n",
    "backend_config = KubernetesBackendConfig(\n",
    "    client_configuration=api_client.configuration,\n",
    ")\n",
    "\n",
    "client = TrainerClient(backend_config)\n",
    "print(\"✅ TrainerClient created\")\n",
    "\n",
    "# Get the torch-distributed runtime\n",
    "runtime = client.backend.get_runtime(\"torch-distributed\")\n",
    "print(f\"✅ Using runtime: {runtime.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5d84c",
   "metadata": {},
   "source": [
    "## Submit the Training Job with TransformersTrainer\n",
    "\n",
    "Now we create and submit the distributed training job. The `TransformersTrainer` wraps your training function and handles all the distributed training setup.\n",
    "\n",
    "### Job Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `num_nodes` | 2 | Number of GPU nodes for distributed training |\n",
    "| `nvidia.com/gpu` | 1 | GPUs per node |\n",
    "| `cpu` | 4 | CPU cores per node |\n",
    "| `memory` | 16Gi | Memory per node |\n",
    "| `enable_progression_tracking` | True | Enable real-time progress monitoring (default) |\n",
    "| `metrics_poll_interval_seconds` | 30 | How often the dashboard polls for metrics |\n",
    "\n",
    "### PVC Mounting\n",
    "\n",
    "We use `PodTemplateOverrides` to mount the shared PVC in training pods:\n",
    "- **Mount Path:** `/opt/app-root/src` - Where training pods access model, data, and checkpoints\n",
    "- **PVC Name:** `shared` - The ReadWriteMany PVC containing our data\n",
    "\n",
    "This ensures all training nodes can access the same model weights, dataset, and can write checkpoints to a shared location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer.options.kubernetes import (\n",
    "    ContainerOverride,\n",
    "    PodSpecOverride,\n",
    "    PodTemplateOverride,\n",
    "    PodTemplateOverrides,\n",
    ")\n",
    "\n",
    "# Create TransformersTrainer with progress tracking (enabled by default)\n",
    "# Checkpointing is handled in the training function - saves to shared PVC\n",
    "trainer = TransformersTrainer(\n",
    "    func=train_func,\n",
    "    num_nodes=2,\n",
    "    resources_per_node={\n",
    "        \"nvidia.com/gpu\": 1,\n",
    "        \"cpu\": \"4\",\n",
    "        \"memory\": \"16Gi\",\n",
    "    },\n",
    "    # Progress tracking is enabled by default\n",
    "    enable_progression_tracking=True,\n",
    "    metrics_poll_interval_seconds=30,\n",
    ")\n",
    "\n",
    "# Submit the training job with PVC mount for model, data, and checkpoints\n",
    "job_name = client.train(\n",
    "    trainer=trainer,\n",
    "    runtime=runtime,\n",
    "    options=[\n",
    "        PodTemplateOverrides(\n",
    "            PodTemplateOverride(\n",
    "                target_jobs=[\"node\"],\n",
    "                spec=PodSpecOverride(\n",
    "                    volumes=[\n",
    "                        {\n",
    "                            \"name\": \"shared\",\n",
    "                            \"persistentVolumeClaim\": {\"claimName\": PVC_NAME},\n",
    "                        },\n",
    "                    ],\n",
    "                    containers=[\n",
    "                        ContainerOverride(\n",
    "                            name=\"node\",\n",
    "                            volume_mounts=[\n",
    "                                {\"name\": \"shared\", \"mountPath\": TRAINING_POD_PATH},\n",
    "                            ],\n",
    "                        )\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "print(f\"✅ Training job submitted: {job_name}\")\n",
    "print(\"📊 Progress tracking: ENABLED (auto-injected by TransformersTrainer)\")\n",
    "print(f\"💾 Checkpoints saved to: {TRAINING_CHECKPOINTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35992cdb",
   "metadata": {},
   "source": [
    "## Monitor Training Progress\n",
    "\n",
    "With progress tracking enabled, you can monitor your training job in real-time. The `TransformersTrainer` automatically exposes metrics via an HTTP endpoint that the Kubeflow controller polls.\n",
    "\n",
    "### Viewing Progress\n",
    "\n",
    "There are two ways to monitor progress:\n",
    "\n",
    "1. **OpenShift AI Dashboard** - Navigate to your TrainJob in the UI to see real-time progress bars and metrics\n",
    "2. **SDK (below)** - Use the `TrainerClient` to programmatically query job status and progress\n",
    "\n",
    "### Available Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `progressPercentage` | Overall completion percentage (0-100) |\n",
    "| `currentStep` / `totalSteps` | Training step progress |\n",
    "| `currentEpoch` / `totalEpochs` | Epoch progress |\n",
    "| `estimatedRemainingSeconds` | Estimated time to completion |\n",
    "| `trainMetrics.loss` | Current training loss value |\n",
    "| `trainMetrics.learning_rate` | Current learning rate |\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Each training pod runs an HTTP server on port 28080\n",
    "2. The server exposes metrics as JSON at the `/` endpoint\n",
    "3. The Kubeflow controller polls this endpoint every 30 seconds (configurable)\n",
    "4. Metrics are written to the TrainJob's status annotations\n",
    "5. Dashboard and SDK read these annotations to display progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e665bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_progress(job):\n",
    "    \"\"\"Format job progress for display.\"\"\"\n",
    "    if not job:\n",
    "        return \"Job not found\"\n",
    "\n",
    "    status = job.get(\"status\", \"Unknown\")\n",
    "    progress = job.get(\"progress\", {})\n",
    "\n",
    "    output = f\"Status: {status}\"\n",
    "\n",
    "    if progress:\n",
    "        pct = progress.get(\"progressPercentage\", 0)\n",
    "        current_step = progress.get(\"currentStep\", 0)\n",
    "        total_steps = progress.get(\"totalSteps\", 0)\n",
    "        current_epoch = progress.get(\"currentEpoch\", 0)\n",
    "        total_epochs = progress.get(\"totalEpochs\", 0)\n",
    "        eta = progress.get(\"estimatedRemainingSeconds\", 0)\n",
    "\n",
    "        output += f\" | Progress: {pct:.1f}%\"\n",
    "        output += f\" | Steps: {current_step}/{total_steps}\"\n",
    "        output += f\" | Epoch: {current_epoch}/{total_epochs}\"\n",
    "\n",
    "        if eta and eta > 0:\n",
    "            mins, secs = divmod(int(eta), 60)\n",
    "            output += f\" | ETA: {mins}m {secs}s\"\n",
    "\n",
    "        metrics = progress.get(\"trainMetrics\", {})\n",
    "        if metrics:\n",
    "            loss = metrics.get(\"loss\", 0)\n",
    "            if loss and loss > 0:\n",
    "                output += f\" | Loss: {loss:.4f}\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"📊 Monitoring training progress...\")\n",
    "print(\"(Press Ctrl+C to stop monitoring)\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        jobs = client.list_jobs()\n",
    "        job = next((j for j in jobs if j.get(\"name\") == job_name), None)\n",
    "\n",
    "        if job:\n",
    "            status = job.get(\"status\", \"\")\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] {format_progress(job)}\")\n",
    "\n",
    "            if status in [\"Succeeded\", \"Failed\", \"Error\"]:\n",
    "                print(f\"\\n✅ Job finished with status: {status}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Waiting for job to start...\")\n",
    "\n",
    "        time.sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️ Monitoring stopped by user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547474d",
   "metadata": {},
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "After training completes, we load the fine-tuned model from the checkpoint saved on the shared PVC.\n",
    "\n",
    "### Checkpoint Structure\n",
    "\n",
    "The training function saves checkpoints with this structure:\n",
    "```\n",
    "/opt/app-root/src/shared/checkpoints/progress-tracking/\n",
    "├── checkpoint-20/     # Intermediate checkpoint at step 20\n",
    "├── checkpoint-32/     # Checkpoint at final step\n",
    "└── final/             # Final merged model ready for inference\n",
    "```\n",
    "\n",
    "### Testing the Model\n",
    "\n",
    "We'll load the fine-tuned model and test it with an instruction prompt using the same format as the training data:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "<your instruction here>\n",
    "\n",
    "### Response:\n",
    "<model generates response>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c43b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_recent_checkpoint(output_dir):\n",
    "    \"\"\"Find the most recently created checkpoint directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        raise FileNotFoundError(f\"Output directory not found: {output_dir}\")\n",
    "\n",
    "    checkpoint_dirs = [\n",
    "        os.path.join(output_dir, d)\n",
    "        for d in os.listdir(output_dir)\n",
    "        if os.path.isdir(os.path.join(output_dir, d))\n",
    "        and (d.startswith(\"checkpoint-\") or d == \"final\")\n",
    "    ]\n",
    "\n",
    "    if not checkpoint_dirs:\n",
    "        raise FileNotFoundError(f\"No checkpoints found in {output_dir}\")\n",
    "\n",
    "    # Prefer 'final' if it exists\n",
    "    final_path = os.path.join(output_dir, \"final\")\n",
    "    if final_path in checkpoint_dirs:\n",
    "        return final_path\n",
    "\n",
    "    return max(checkpoint_dirs, key=os.path.getctime)\n",
    "\n",
    "\n",
    "print(\"✅ Checkpoint utility defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc49102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load the trained model\n",
    "final_checkpoint = find_most_recent_checkpoint(CHECKPOINTS_PATH)\n",
    "print(f\"📂 Loading checkpoint from: {final_checkpoint}\")\n",
    "\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    final_checkpoint, trust_remote_code=True\n",
    ")\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    final_checkpoint,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully\")\n",
    "print(f\"📊 Model parameters: {trained_model.num_parameters():,}\")\n",
    "\n",
    "# Test the model\n",
    "test_prompt = \"### Instruction:\\nExplain what machine learning is in one sentence.\\n\\n### Response:\"\n",
    "\n",
    "print(\"\\n📝 Testing model with prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\n🤖 Model response:\")\n",
    "\n",
    "inputs = trained_tokenizer(test_prompt, return_tensors=\"pt\").to(trained_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "response = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response.replace(test_prompt, \"\").strip())\n",
    "\n",
    "print(\"\\n✅ Model test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2127db",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the training job and free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eccd277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the training job\n",
    "client.delete_job(name=job_name)\n",
    "print(f\"✅ Job {job_name} deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b89f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "gc.collect()\n",
    "print(\"✅ Resources freed, CUDA cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415804b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully completed a distributed fine-tuning job with real-time progress tracking on OpenShift AI.\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| ✅ Model Download | Downloaded Qwen 2.5 1.5B Instruct to shared PVC |\n",
    "| ✅ Dataset Preparation | Processed Stanford Alpaca dataset for instruction-tuning |\n",
    "| ✅ Distributed Training | Ran 2-node distributed training with PyTorch DDP |\n",
    "| ✅ Progress Tracking | Monitored real-time metrics via SDK and Dashboard |\n",
    "| ✅ Checkpointing | Saved model checkpoints to shared PVC |\n",
    "| ✅ Model Testing | Loaded and tested the fine-tuned model |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **TransformersTrainer** automatically instruments your HuggingFace `Trainer` with:\n",
    "   - `KubeflowProgressCallback` for real-time metrics (HTTP endpoint on port 28080)\n",
    "   - Progress visible in OpenShift AI Dashboard without any code changes\n",
    "\n",
    "2. **Shared PVC Strategy** for distributed training:\n",
    "   - Pre-download model and dataset to shared RWX PVC from workbench\n",
    "   - Training pods mount the same PVC and access data locally (offline mode)\n",
    "   - Checkpoints are written to shared storage for durability\n",
    "\n",
    "3. **Supported Trainers**:\n",
    "   - `transformers.Trainer` - Standard HuggingFace trainer\n",
    "   - `trl.SFTTrainer` - TRL's supervised fine-tuning trainer\n",
    "\n",
    "### TransformersTrainer Quick Reference\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `func` | Training function using `transformers.Trainer` | Required |\n",
    "| `num_nodes` | Number of distributed training nodes | Required |\n",
    "| `resources_per_node` | GPU, CPU, memory per node | Required |\n",
    "| `enable_progression_tracking` | Enable real-time metrics server | `True` |\n",
    "| `metrics_poll_interval_seconds` | How often controller polls metrics | `30` |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Scale Up:** Increase `num_nodes` for larger models or datasets\n",
    "- **Use LoRA:** Add PEFT/LoRA for memory-efficient fine-tuning\n",
    "- **Try Other Models:** This pattern works with any HuggingFace model\n",
    "- **Enable JIT Checkpointing:** Use `enable_jit_checkpoint=True` for automatic checkpoint saving on preemption\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Kubeflow Trainer Documentation](https://www.kubeflow.org/docs/components/trainer/)\n",
    "- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)\n",
    "- [Stanford Alpaca Dataset](https://huggingface.co/datasets/tatsu-lab/alpaca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
